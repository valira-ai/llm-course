{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Sodobna obdelava naravnega jezika: BERT prek praktičnih primerov"
      ],
      "metadata": {
        "id": "UzycLR-jESOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zaznava imenskih entitet\n",
        "\n",
        "Praktični del 3. delavnice v sklopu Akademije umetne inteligence za poslovne aplikacije.\n",
        "\n",
        "V tej beležki se bomo naučili uporabiti *BERT-a* oziroma podobne modele za zaznavo imenskih entitet. Model bo lahko vsako besedo v besedilu klasificiral v različne kategorije, kot so oseba, organizacija, lokacija, itd."
      ],
      "metadata": {
        "id": "y1X6YajuEUr6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Najprej si uredimo dostop do GPU-ja v tej Colab seji:\n",
        "- `Edit -> Notebook settings -> Hardware accelerator` mora biti nastavljen na enega izmed GPU-jev.\n",
        "- po potrebi se ponovno poveženo z gumbom `Connect` v desnem zgornjem kotu."
      ],
      "metadata": {
        "id": "7YG6PkUqFnz3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dhr-myFbEN1e"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install datasets evaluate transformers[torch] seqeval colorama"
      ],
      "metadata": {
        "id": "vPRLh45rFVcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForTokenClassification,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")"
      ],
      "metadata": {
        "id": "DU1zx3MuFtCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prenesimo podatke in poglejmo nekaj primerov:"
      ],
      "metadata": {
        "id": "JDgN9A4uF_Cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"conll2003\")"
      ],
      "metadata": {
        "id": "ur5WRdzYF_d1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "KKJR333UGF74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][0][\"tokens\"]"
      ],
      "metadata": {
        "id": "pWAc06plGGbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][0][\"ner_tags\"]"
      ],
      "metadata": {
        "id": "xvNbI8b7QBYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"].features[\"ner_tags\"]"
      ],
      "metadata": {
        "id": "u6zls9koQDKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zakaj ločujemo začetek entitete (B-PER vs I-PER)? Da lahko ločimo med večimi zaporednimi entitetami, e.g.:\n",
        "\n",
        "To sta Luka, Andrej."
      ],
      "metadata": {
        "id": "c9QX6sptQ6p1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = dataset[\"train\"][0][\"tokens\"]\n",
        "labels = dataset[\"train\"][0][\"ner_tags\"]\n",
        "label_names = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
        "\n",
        "sentence_str = \"\"\n",
        "label_str = \"\"\n",
        "for word, label in zip(words, labels):\n",
        "    max_length = max(len(word), len(label_names[label]))\n",
        "    sentence_str += word + \" \" * (max_length - len(word) + 1)\n",
        "    label_str += label_names[label] + \" \" * (max_length - len(label_names[label]) + 1)\n",
        "\n",
        "print(sentence_str)\n",
        "print(label_str)"
      ],
      "metadata": {
        "id": "bY6cto8URPSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naš cilj v tej beležki je \"finetunanje\" (dodatno učenje) že obstoječega BERT modela, da bo klasificiral vsako besedo v besedilu v neko kategorijo."
      ],
      "metadata": {
        "id": "fz1WFY32UAck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Priprava podatkov"
      ],
      "metadata": {
        "id": "lnNczs4MUMRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\", model_max_length=512)"
      ],
      "metadata": {
        "id": "Ac1wVOVZUhE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def align_labels_with_tokens(labels, word_ids):\n",
        "    new_labels = []\n",
        "    current_word = None\n",
        "    for word_id in word_ids:\n",
        "        if word_id != current_word:\n",
        "            current_word = word_id\n",
        "            label = -100 if word_id is None else labels[word_id]\n",
        "            new_labels.append(label)\n",
        "        elif word_id is None:\n",
        "            new_labels.append(-100)\n",
        "        else:\n",
        "            label = labels[word_id]\n",
        "            if label % 2 == 1:\n",
        "                label += 1\n",
        "            new_labels.append(label)\n",
        "\n",
        "    return new_labels\n",
        "\n",
        "\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
        "    )\n",
        "    all_labels = examples[\"ner_tags\"]\n",
        "    new_labels = []\n",
        "    for i, labels in enumerate(all_labels):\n",
        "        word_ids = tokenized_inputs.word_ids(i)\n",
        "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = new_labels\n",
        "    return tokenized_inputs"
      ],
      "metadata": {
        "id": "ESgXnfnGUTaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_tokenized = dataset.map(\n",
        "    tokenize_and_align_labels,\n",
        "    batched=True\n",
        ")"
      ],
      "metadata": {
        "id": "5ozvFC7rUcZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model"
      ],
      "metadata": {
        "id": "LlwnnrLPVweH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {i: label for i, label in enumerate(dataset[\"train\"].features[\"ner_tags\"].feature.names)}\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"distilbert/distilbert-base-uncased\",\n",
        "    id2label=id2label,\n",
        "    label2id={v: k for k, v in id2label.items()},\n",
        ")"
      ],
      "metadata": {
        "id": "lHRDZWWmVb9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/\",\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=64,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")"
      ],
      "metadata": {
        "id": "rhox-HWWVrRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model.cuda(),\n",
        "    args=training_args,\n",
        "    train_dataset=dataset_tokenized[\"train\"],\n",
        "    eval_dataset=dataset_tokenized[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorForTokenClassification(tokenizer=tokenizer),\n",
        ")"
      ],
      "metadata": {
        "id": "03Clqu_CVtsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "F1q15UN0VvH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evalvirajmo naučen model"
      ],
      "metadata": {
        "id": "ukyHSrWviLRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import evaluator\n",
        "from transformers import AutoModelForTokenClassification, pipeline"
      ],
      "metadata": {
        "id": "_6D1WwvzbtFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/bert-ner\n",
        "!gdown -O /content/bert-ner/config.json https://drive.google.com/uc?id=1uAfr57dXkz9r-fIIT93u_miKiCMb1YmK\n",
        "!gdown -O /content/bert-ner/model.safetensors https://drive.google.com/uc?id=14f1skonBD3LbRPFPrjY3haBxjvL7trrf"
      ],
      "metadata": {
        "id": "t1qp7B0xbz8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {i: label for i, label in enumerate(dataset[\"train\"].features[\"ner_tags\"].feature.names)}\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"/content/bert-ner\",\n",
        "    id2label=id2label,\n",
        "    label2id={v: k for k, v in id2label.items()},\n",
        ")"
      ],
      "metadata": {
        "id": "Ky2GslizcUbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task_evaluator = evaluator(\"token-classification\")"
      ],
      "metadata": {
        "id": "jP0U2QpucT9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = task_evaluator.compute(\n",
        "    model_or_pipeline=model.cuda(),\n",
        "    tokenizer=tokenizer,\n",
        "    data=dataset_tokenized[\"test\"]\n",
        ")"
      ],
      "metadata": {
        "id": "rbltWeoccuJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results"
      ],
      "metadata": {
        "id": "H0ZUXRltdNmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Eksperimentirajmo"
      ],
      "metadata": {
        "id": "u7ffzGnldU7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from colorama import Fore"
      ],
      "metadata": {
        "id": "YAoBf8GXfKW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "cqOlXbV5fnLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type2color = {\n",
        "    \"PER\": Fore.GREEN,\n",
        "    \"ORG\": Fore.BLUE,\n",
        "    \"LOC\": Fore.RED,\n",
        "    \"MISC\": Fore.YELLOW\n",
        "}\n",
        "\n",
        "def detect_and_print_entities(text: str) -> None:\n",
        "  ents = ner_pipeline(text)\n",
        "  if len(ents) == 0:\n",
        "    print(text)\n",
        "    return\n",
        "\n",
        "  print(text[:ents[0][\"start\"]], end=\"\")\n",
        "  for i in range(len(ents)):\n",
        "    print(type2color[ents[i][\"entity\"][2:]] + text[ents[i][\"start\"]:ents[i][\"end\"]], end=\"\")\n",
        "    if i == len(ents) - 1:\n",
        "      print(Fore.BLACK + text[ents[i][\"end\"]:])\n",
        "    else:\n",
        "      print(Fore.BLACK + text[ents[i][\"end\"]:ents[i + 1][\"start\"]], end=\"\")"
      ],
      "metadata": {
        "id": "a1PqnUR-e9r3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detect_and_print_entities(\"Lewis Hamilton's quest for the all-time record of Formula 1 wins was put on hold when he was hit with penalties at the Russian Grand Prix. Hamilton's Mercedes team-mate Valtteri Bottas dominated after the world champion was given a 10-second penalty for doing two illegal practice starts. Bottas was on the better strategy - starting on the medium tyres while Hamilton was on softs after a chaotic qualifying session for the Briton - and was tracking Hamilton in the early laps waiting for the race to play out. Behind the top three, Racing Point's Sergio Perez and Renault's Daniel Ricciardo had equally lonely races, the Australian having sufficient pace to overcome a five-second penalty for failing to comply with rules regarding how to rejoin the track when a car runs wide at Turn Two. Ferrari's Charles Leclerc made excellent use of a long first stint on the medium tyres to vault up from 11th on the grid to finish sixth, ahead of the second Renault of Esteban Ocon, the Alpha Tauris of Daniil Kvyat and Pierre Gasly and Alexander Albon's Red Bull. What's next? The Eifel Grand Prix on 11 October as the Nurburgring returns to the F1 calendar for the first time since 2013. The 24-hour touring car race there this weekend has been hit with miserable wet and wintery conditions in the Eifel mountains. Will F1 face the same?\")"
      ],
      "metadata": {
        "id": "MJzZ7wf_fLVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detect_and_print_entities(\"Sir David Attenborough has broken Jennifer Aniston's record for the fastest time to reach a million followers on Instagram. At 94 years young, the naturalist's follower count raced to seven figures in four hours 44 minutes on Thursday, according to Guinness World Records. His debut post said: \\'Saving our planet is now a communications challenge.\\' Last October, Friends star Aniston reached the milestone in five hours and 16 minutes. Sir David's Instagram debut precedes the release of a book and a Netflix documentary, both titled A Life On Our Planet.\")"
      ],
      "metadata": {
        "id": "EO7Y_4kAhoHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detect_and_print_entities(\"Using Lidar, in 2016 the Foundation for Maya Cultural and Natural Heritage launched the largest archaeological survey ever undertaken of the Maya lowlands. In the first phase, whose results were published in 2018, they mapped 2,100km of the Maya Biosphere Reserve. Their hope in the further phases – the second one of which took place in summer 2019, while I was there – is to triple the coverage area. That would make the project the largest Lidar survey not only in Central America, but in the world.\")"
      ],
      "metadata": {
        "id": "BpdWmWSDhsv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ali lahko za to uporabimo tudi ChatGPT?"
      ],
      "metadata": {
        "id": "OYv6o-fUeVAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -q openai"
      ],
      "metadata": {
        "id": "l2pyYjLbeUzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import requests"
      ],
      "metadata": {
        "id": "QbH50lJ3glls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = \"\""
      ],
      "metadata": {
        "id": "PaX8iL9HgnA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_model(msg: str, temperature: float = 1., top_p: float = 1., model: str = \"gpt-3.5-turbo\", system: str = None):\n",
        "    URL = \"https://api.openai.com/v1/chat/completions\"\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": msg}] if system is None else [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": msg}]\n",
        "\n",
        "    payload = {\n",
        "      \"model\": model,\n",
        "      \"messages\": messages,\n",
        "      \"temperature\" : temperature,\n",
        "      \"top_p\":top_p,\n",
        "    }\n",
        "\n",
        "    headers = {\n",
        "      \"Content-Type\": \"application/json\",\n",
        "      \"Authorization\": f\"Bearer {openai.api_key}\"\n",
        "    }\n",
        "\n",
        "    response = requests.post(URL, headers=headers, json=payload, stream=False)\n",
        "    return response.json()['choices'][0]['message']['content'].strip()"
      ],
      "metadata": {
        "id": "96hAz0wOgprB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "call_model(\"What's the capital of Slovenia?\")"
      ],
      "metadata": {
        "id": "wbvBiZ3lgrXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iz zgornjih primerov poskusite izvleči entitete z uporabo ChatGPT-ja."
      ],
      "metadata": {
        "id": "stiDzaSFgr0F"
      }
    }
  ]
}